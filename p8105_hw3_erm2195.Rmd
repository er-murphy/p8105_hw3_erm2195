---
title: "P8105 Homework 3"
output: github_document
date: "2023-10-10"
---

This homework includes figures; the readability of embedded plots (e.g. font sizes, axis labels, titles) will be assessed!

# Library Setup
Loading the `tidyverse` and `p8105 datasets` libraries for later use. Also establishing some document-wide theme and visualization output settings.

```{r load_libraries, include = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 1

## Reading in Dataset

```{r}
data("instacart")

instacart = 
  as_tibble(instacart)
```

## Describing the `instacart` dataset
The `instacart` dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. It contains information about the items in instacart orders from `r instacart |> select(user_id) |> distinct() |> count()` different individuals in 2017. The dataset includes variables with details about the user placing the order, the date and time of the order itself, and specifics on the items being ordered, including their name and physical and department location in the store. In total, there are `r instacart |> select(product_id) |> distinct() |> count()` separate products included in the dataset. On average, each order was placed `r instacart |> pull(days_since_prior_order) |> mean() |> round()` days after that individual's prior order. `r 100 *(instacart |> pull(reordered) |> mean() |> round(digits = 4))`% of products were reordered.

## Creating Table of Most Commonly Used Aisles
To see how many aisles there are, and which aisles have the most items ordered from them, we can create a table. Below is a table summarizing the number of items ordered from aisle.

```{r}
instacart |> 
  count(aisle) |> 
  arrange(desc(n))
```

In total, there are `r instacart |> count(aisle) |> arrange(desc(n)) |> nrow()` aisles, and the two most commonly ordered-from aisles are `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(1) |> pull(aisle)` and `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(2) |> pull(aisle)`.

## Plotting Items Ordered from Each Aisle
Now, generating a plot displaying the number of items ordered from each aisle, limited to aisles with more than 10,000 items ordered. We can do this by filtering the dataset based on order count. Arranging the plot by number of orders so that the aisles generate a nice curve. 

```{r}
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(
    aisle = fct_reorder(aisle, n)
  ) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(
    title = "Total number of items ordered from each aisle",
    x = "Aisle",
    y = "Item count"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
  
```

As the plot reveals, the majority of aisles have less than 40,000 items ordered, and the most popular aisles of `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(1) |> pull(aisle)` and `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(2) |> pull(aisle)` have more than 3 times that number. 

## Creating Table of Popular Baking, Dog Food, and Vegetable Items
Creating a table displaying the 3 most popular items in the "baking ingredients", "dog food care", and "packaged vegetables fruits" aisles, and how often each item is ordered. Doing this by filtering to only include items from the 3 categories of interest, then counting up how many times each item was ordered. Then, I created a new `rank` variable of each item's place in the ranking for its category, and limited the table to items with rank 1-3.  

```{r}
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(
    rank = min_rank(desc(n))
  ) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |> 
  knitr::kable()

```

## Creating Table of Mean Purchase Time of Apples and Ice Cream
Making a table of the mean time when Pink Lady apples and coffee ice cream are ordered, per day of the week. Because I want to make the table easily readable for humans, I will use `pivot_wider` to make it "untidy" and have each day of the week in a different column. 

```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(avg_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = avg_hour
  ) |>
  knitr::kable(digits = 2)
```

From the table, we can see that coffee ice cream is most often purchased between noon and 4pm, while pink lady apples are typically ordered from 11am to 3pm. 

# Problem 2

## Reading in Dataset

```{r}
data("brfss_smart2010")

brfss = 
  as_tibble(brfss_smart2010) |> 
  janitor::clean_names()
```

## Cleaning the `brfss` Dataset
Cleaning up the dataset ahead of analysis:

* Filtering to only include responses to questions from the "Overall Health" topic
* Filtering to include responses ranging from "Excellent" to "Poor". These responses have values between 1 and 5 for the` display_order` variable, so filtering to only include those values and exclude observations that are missing a `response` variable value 
* Renaming variables to more clearly define their contents
* Organizing `responses` as a factor taking levels ordered from "Poor" to "Excellent"

```{r}

brfss =
  rename(brfss,
    state = locationabbr, 
    county = locationdesc, 
    response_count = sample_size, 
    response_prevalence = data_value, 
    response_rank = display_order,
    response_id = respid
  ) |> 
  filter(topic == "Overall Health" & response_rank <= 5 ) |> 
  mutate(
    response = factor(response, 
                      levels = c("Poor", "Fair", "Good", "Very Good", "Excellent"),
                      ordered = TRUE)
  )

```

# States Observed at 7 or More Locations
In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r}
brfss |> 
  filter(year == 2002) |>
  group_by(state) |> 
  select(county) |> 
  distinct() |> 
  count() |> 
  arrange(desc(n)) |> 
  filter(n >= 7)


brfss |> 
  filter(year == 2010) |> 
  group_by(state) |> 
  select(county) |> 
  distinct() |> 
  count() |> 
  arrange(desc(n)) |> 
  filter(n >= 7)
  
```

In 2002, these `r brfss |> filter(year == 2002) |> group_by(state) |> select(county) |> distinct() |> count() |> arrange(desc(n)) |> filter(n >= 7) |> nrow()` states were observed at 7 or more locations: `r brfss |> filter(year == 2002) |> group_by(state) |> select(county) |> distinct() |> count() |> arrange(desc(n)) |> filter(n >= 7) |> pull(state)`.

In comparison, in 2010, these `r brfss |> filter(year == 2010) |> group_by(state) |> select(county) |> distinct() |> count() |> arrange(desc(n)) |> filter(n >= 7) |> nrow()` states were observed at 7 or more locations: `r brfss |> filter(year == 2010) |> group_by(state) |> select(county) |> distinct() |> count() |> arrange(desc(n)) |> filter(n >= 7) |> pull(state)`.

`r (brfss |> filter(year == 2010) |> group_by(state) |> select(county) |> distinct() |> count() |> arrange(desc(n)) |> filter(n >= 7) |> nrow()) - (brfss |> filter(year == 2002) |> group_by(state) |> select(county) |> distinct() |> count() |> arrange(desc(n)) |> filter(n >= 7) |> nrow())` additional states were observed at 7 or more locations in 2010 compared to 2002. Of those observed in 2002, only Connecticut wasn't similarly observed at 7 or more locations in 2010.

# Plotting Prevalence of Excellent Responses Across State Locations
Constructing a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 

```{r}
brfss_excellent =
  filter(brfss, response == "Excellent") |> 
  group_by(year, state) |> 
  mutate(
    avg_excellence = mean(response_prevalence, na.rm = TRUE)
  ) |> 
  select(year, state, avg_excellence) |> 
  group_by(year) |> 
  distinct(state, .keep_all = TRUE)

```

Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
ggplot(brfss_excellent, aes(x = year, y = avg_excellence, group = state, color = state)) +
  geom_line(alpha = .5) +
  labs(
    title = 'Prevalence of "Excellent" Response by State: 2002 to 2010',
    x = "Year",
    y = 'Prevalence of "Excellent"',
    color = "State",
    caption = "Data from from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART)"
  )

```

From the spaghetti plot, we can see that West Virginia consistently has the lowest, if not nearly the lowest, average prevalence of "Excellent" responses to questions asking about overall health. In contrast, DC and Connecticut are generally in the top of the pack for prevalence of "Excellent" responses. Across the country as a whole, it appears that there's a slight decline in the prevalence of "Excellent" responses from 2002 to 2010. 

# Problem 3

Accelerometers can measure MIMS in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations. This problem uses accelerometer data collected on 250 participants in the NHANES study. Variables MIMS are the MIMS values for each minute of a 24-hour day starting at midnight.

## Reading in Dataset

```{r}
mims_accel = 
  read_csv("Data/nhanes_accel.csv") |> 
  as_tibble() |> 
  janitor::clean_names()

mims_covar = 
  read_csv("Data/nhanes_covar.csv", skip = 4) |> 
  as_tibble() |> 
  janitor::clean_names()

```

## Cleaning the `brfss` Dataset

Tidying and merging the two datasets. Specifically:

* Maintaining all originally observed variables
* Excluding participants younger than 21 years old

* Final dataset should include all originally observed variables
* Exclude participants less than 21 years of age


* Encode data with reasonable variable classes (i.e. not numeric, and using factors with the ordering of tables and plots in mind)
* Exclude participants missing demographic data


** exclude poeple missing any demographic data in mims_covar? Or just those that don't appear in mims_covar at all?


```{r}

mims_covar = 
  filter(mims_covar, age >= 21) |> 
  mutate(
    sex = as.factor(case_match(sex,
                               1 ~"male",
                               2 ~ "female")),
    education = as.factor(case_match(education,
                                     1 ~ "less than high school",
                                     2 ~ "high school equivalent",
                                     3 ~ "more than high school")),
    education = factor(education, 
                       levels = c("less than high school", "high school equivalent", "more than high school"),
                       ordered = TRUE)
  )
```



## Visualizing the Age Distributions of Education Categories, by Sex
Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. Comment on these items.


## Plotting Total Activity and Age, by Sex and Education
Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences. Comment on your plot.


## Plotting Activity Time Courses, by Education
Accelerometer data allows the inspection activity over the course of the day. Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. Describe in words any patterns or conclusions you can make based on this graph; including smooth trends may help identify differences.












