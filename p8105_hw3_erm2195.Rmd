---
title: "P8105 Homework 3"
output: github_document
date: "2023-10-10"
---

This homework includes figures; the readability of embedded plots (e.g. font sizes, axis labels, titles) will be assessed!

# Library Setup
Loading the `tidyverse` and `p8105 datasets` libraries for later use. Also establishing some document-wide theme and visualization output settings.

```{r load_libraries, include = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 1

## Reading in Dataset

```{r}
data("instacart")

instacart = 
  as_tibble(instacart)
```

## Describing the `instacart` dataset
The `instacart` dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. It contains information about the items in instacart orders from `r instacart |> select(user_id) |> distinct() |> count()` different individuals in 2017. The dataset includes variables with details about the user placing the order, the date and time of the order itself, and specifics on the items being ordered, including their name and physical and department location in the store. In total, there are `r instacart |> select(product_id) |> distinct() |> count()` separate products included in the dataset. On average, each order was placed `r instacart |> pull(days_since_prior_order) |> mean() |> round()` days after that individual's prior order. `r 100 *(instacart |> pull(reordered) |> mean() |> round(digits = 4))`% of products were reordered.

## Creating Table of Most Commonly Used Aisles
To see how many aisles there are, and which aisles have the most items ordered from them, we can create a table. Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart |> 
  count(aisle) |> 
  arrange(desc(n))
```

In total, there are `r instacart |> count(aisle) |> arrange(desc(n)) |> nrow()` aisles, and the two most commonly ordered-from aisles are `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(1) |> pull(aisle)` and `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(2) |> pull(aisle)`.

## Plotting Items Ordered from Each Aisle
Now, generating a plot displaying the number of items ordered from each aisle, limited to aisles with more than 10,000 items ordered. We can do this by filtering the dataset based on order count. Arranging the plot by number of orders so that the aisles generate a nice curve. 

```{r}
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(
    aisle = fct_reorder(aisle, n)
  ) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(
    title = "Total number of items ordered from each aisle",
    x = "Aisle",
    y = "Item count"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
  
```

As the plot reveals, the majority of aisles have less than 40,000 items ordered, and the most popular aisles of `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(1) |> pull(aisle)` and `r instacart |> count(aisle) |> arrange(desc(n)) |> slice(2) |> pull(aisle)` have more than 3 times that number. 

## Creating Table of Popular Baking, Dog Food, and Vegetable Items
Creating a table displaying the 3 most popular items in the "baking ingredients", "dog food care", and "packaged vegetables fruits" aisles, and how often each item is ordered. Doing this by filtering to only include items from the 3 categories of interest, then counting up how many times each item was ordered. Then, I created a new `rank` variable of each item's place in the ranking for its category, and limited the table to items with rank 1-3.  

```{r}
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(
    rank = min_rank(desc(n))
  ) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |> 
  knitr::kable()

```

## Creating Table of Mean Purchase Time of Apples and Ice Cream
Making a table of the mean time when Pink Lady apples and coffee ice cream are ordered, per day of the week. Because I want to make the table easily readable for humans, I will use `pivot_wider` to make it "untidy" and have each day of the week in a different column. 

```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(avg_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = avg_hour
  ) |>
  knitr::kable(digits = 2)
```

From the table, we can see that coffee ice cream is most often purchased between noon and 4pm, while pink lady apples are typically ordered from 11am to 3pm. 

# Problem 2

## Reading in Dataset

```{r}
data("brfss_smart2010")

brfss = 
  as_tibble(brfss_smart2010) |> 
  janitor::clean_names()
```

## Cleaning the `brfss` dataset
Cleaning up the dataset ahead of analysis:

* Filtering to only include responses to questions from the "Overall Health" topic
* Filtering to include responses ranging from "Excellent" to "Poor". These responses have values between 1 and 5 for the` display_order` variable, so filtering to only include those values and exclude observations that are missing a `response` variable value 
* Renaming variables to more clearly define their contents
* Organizing `responses` as a factor taking levels ordered from "Poor" to "Excellent"

```{r}

brfss =
  filter(brfss, topic == "Overall Health" & is.na(response) == FALSE & display_order <= 5 )|> 
  rename(
    state = locationabbr, 
    county = locationdesc, 
    response_count = sample_size, 
    response_prevalence = data_value, 
    response_rank = display_order,
    response_id = respid
    ) |> 
  mutate(
    response = factor(response, 
                      levels = c("Poor", "Fair", "Good", "Very Good", "Excellent"),
                      ordered = TRUE)
  )

```

Using this dataset, do or answer the following (commenting on the results of each):

In 2002, which states were observed at 7 or more locations? What about in 2010?
Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.










